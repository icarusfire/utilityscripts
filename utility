# Refactor: separate **source loading** from **table parsing**
#
# ┌────────────────────────┐
# │ 1. load_docx_from_path │  ← I/O boundary (disk)          (replaceable)
# └────────────────────────┘
#                │ returns python‑docx Document
# ┌────────────────────────┐
# │ 2. parse_release_table │  ← core parser (pure function)  (stable)
# └────────────────────────┘
#
# The parser operates only on a *Document* instance, so switching to a
# REST source tomorrow means replacing the loader while re‑using the parser.

from datetime import datetime
from typing import Tuple, Optional, Dict, List
import pandas as pd
from docx import Document
import unittest
import tempfile
import re
from io import BytesIO

# ---------------------------------------------------------------------
# Constants
# ---------------------------------------------------------------------
TARGET_PREFIXES: Tuple[str, ...] = ("Release Version",
                                    "RT Driver Squad",
                                    "Branch Date")


# ---------------------------------------------------------------------
# Helpers
# ---------------------------------------------------------------------
def _normalise(text: str) -> str:
    """Lower‑case *text* and remove **all** whitespace."""
    return re.sub(r"\s+", "", text.lower())

def get_assignment_group(squad_name: str) -> Optional[str]:
    """
    Return the technical code whose key token is contained inside *squad_name*
    (case-insensitive).  First match wins; returns **None** if nothing fits.
    """
    name_norm = squad_name.lower()
    for token, code in _SQUAD_TO_TECH.items():
        if token in name_norm:
            return code
    return None


def _match_required_prefixes(header_cells: List[str],
                             prefixes: Tuple[str, ...]) -> Optional[Dict[str, int]]:
    """
    Return mapping **prefix → column_index** if every prefix is matched
    (whitespace‑ and case‑insensitive `startswith`); otherwise *None*.
    """
    prefix_to_index: Dict[str, int] = {}
    normalised_headers = [_normalise(cell) for cell in header_cells]
    normalised_prefixes = {_normalise(p): p for p in prefixes}

    for col_idx, header_norm in enumerate(normalised_headers):
        for prefix_norm, canonical_prefix in normalised_prefixes.items():
            if header_norm.startswith(prefix_norm):
                prefix_to_index[canonical_prefix] = col_idx
                break

    if len(prefix_to_index) == len(prefixes):
        return prefix_to_index
    return None


# ---------------------------------------------------------------------
# 1.  SOURCE LOADERS (I/O layer)
# ---------------------------------------------------------------------
def load_docx_from_path(path: str) -> Document:
    """Load a Word document from *path* on disk."""
    return Document(path)


def load_docx_from_bytes(data: bytes) -> Document:
    """Load a Word document from a byte string (e.g., HTTP response)."""
    return Document(BytesIO(data))


# ---------------------------------------------------------------------
# 2.  CORE PARSER (pure function)
# ---------------------------------------------------------------------
def parse_release_table(document: Document,
                        prefixes: Tuple[str, ...] = TARGET_PREFIXES) -> pd.DataFrame:
    """
    Inspect *document* (python‑docx Document) and build a DataFrame from all
    tables that contain the required *prefixes*.

    Header matching is case‑insensitive and whitespace‑insensitive.
    """
    if not document.tables:
        raise ValueError("Document contains no tables.")

    collected_rows: List[Dict[str, str]] = []

    for table in document.tables:
        if not table.rows:
            continue

        header_cells = [cell.text.strip() for cell in table.rows[0].cells]
        prefix_to_index = _match_required_prefixes(header_cells, prefixes)
        if prefix_to_index is None:
            continue  # ignore irrelevant table

        for row in table.rows[1:]:
            collected_rows.append({
                prefix: row.cells[prefix_to_index[prefix]].text.strip()
                for prefix in prefixes
            })

    if not collected_rows:
        raise ValueError("No tables with the required columns were found.")

    df = pd.DataFrame(collected_rows)
    df["Branch Date"] = pd.to_datetime(df["Branch Date"],
                                       format="%d %b %Y",
                                       errors="coerce")
    df.dropna(subset=["Branch Date"], inplace=True)
    return df


# Convenience wrapper preserving old name
def read_release_table(docx_path: str,
                       prefixes: Tuple[str, ...] = TARGET_PREFIXES) -> pd.DataFrame:
    """Load from disk then parse – legacy helper."""
    return parse_release_table(load_docx_from_path(docx_path), prefixes)


# ---------------------------------------------------------------------
# 3.  QUERY FUNCTION
# ---------------------------------------------------------------------
def get_squad_name(df: pd.DataFrame,
                   branch_date: str | datetime) -> Optional[Tuple[str, str]]:
    date_norm = (pd.to_datetime(branch_date, format="%d %b %Y")
                 if isinstance(branch_date, str)
                 else pd.to_datetime(branch_date))
    match = df[df["Branch Date"] == date_norm]
    if match.empty:
        return None
    row = match.iloc[0]
    return row["RT Driver Squad"], row["Release Version"]


# ---------------------------------------------------------------------
# 4.  UNIT TESTS
# ---------------------------------------------------------------------
class _SeparationOfConcernsTests(unittest.TestCase):

    def setUp(self):
        # Build an in‑memory DOCX and also save it to disk
        self.temp_doc = tempfile.NamedTemporaryFile(delete=False, suffix=".docx")
        doc = Document()

        # Relevant table
        tbl = doc.add_table(rows=1, cols=3)
        tbl.rows[0].cells[0].text = "release version"
        tbl.rows[0].cells[1].text = "RT Driver Squad"
        tbl.rows[0].cells[2].text = "branch date (utc)"
        row = tbl.add_row().cells
        row[0].text, row[1].text, row[2].text = "27.0", "Poseidon", "07 Jul 2025"

        # Save to disk and to bytes
        doc.save(self.temp_doc.name)
        buf = BytesIO()
        doc.save(buf)
        self.buf_bytes = buf.getvalue()

    def tearDown(self):
        self.temp_doc.close()

    # --- tests --------------------------------------------------------
    def test_disk_loader(self):
        doc_from_disk = load_docx_from_path(self.temp_doc.name)
        df = parse_release_table(doc_from_disk)
        self.assertEqual(get_squad_name(df, "07 Jul 2025"), ("Poseidon", "27.0"))

    def test_bytes_loader(self):
        doc_from_bytes = load_docx_from_bytes(self.buf_bytes)
        df = parse_release_table(doc_from_bytes)
        self.assertEqual(get_squad_name(df, "07 Jul 2025"), ("Poseidon", "27.0"))


unittest.TextTestRunner().run(unittest.TestLoader().loadTestsFromTestCase(_SeparationOfConcernsTests))
